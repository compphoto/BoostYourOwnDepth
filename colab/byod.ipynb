{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost your own depth\n",
    "This Colab Notebook helps you applying the global boosting method from https://github.com/compphoto/BoostingMonocularDepth \n",
    "on your own depth. It executes a single pass through the merging network, that we use to generate the **double estimation**.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Ensure using a GPU by setting \"Runtime/change runtime type\" to GPU\n",
    "2. Run code section 1\n",
    "3. Put your low-resolution base depth inside '/content/BoostYourOwnDepth/input/low-res'\n",
    "4. (If you want to calculate the R20 resolution for your image, you can do the following:\n",
    "        a.) Put the RGB image into '/content/BoostYourOwnDepth/input/rgb'\n",
    "        b.) Run section 2 to get the optimal input resolution for a given image\n",
    "        c.) Create a depth estimation from your network, with the resolution from b.) as input size\n",
    "    )\n",
    "5. Put your high-resolution depth inside '/content/BoostYourOwnDepth/input/high-res'\n",
    "6. Run code section 3\n",
    "    \n",
    "The results will be generated in '/content/outputs'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1\n",
    "This section is cloning the repository and downloads the weights for the merging network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code section 1\n",
    "\n",
    "# Clone repo\n",
    "!git clone https://github.com/compphoto/BoostYourOwnDepth.git\n",
    "\n",
    "# Downloading merge model weights\n",
    "# !wget https://sfu.ca/~yagiz/CVPR21/latest_net_G.pth\n",
    "!gdown \"1cU2y-kMbt0Sf00Ns4CN2oO9qPJ8BensP&confirm=t\"\n",
    "\n",
    "!mkdir -p /content/BoostYourOwnDepth/pix2pix/checkpoints/mergemodel/\n",
    "!mv /content/latest_net_G.pth /content/BoostYourOwnDepth/pix2pix/checkpoints/mergemodel/\n",
    "\n",
    "!ls /content/BoostYourOwnDepth/pix2pix/checkpoints/mergemodel/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2\n",
    "Run the next section to calculate the R20 resolution for an image of your choice. **IMPORTANT**: This will **NOT** do the actual depth estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code section 2\n",
    "from BoostYourOwnDepth.utils import calculateprocessingres, read_image\n",
    "import os\n",
    "\n",
    "############################################################################\n",
    "#### Set the image name that you want to optimize for, must be present in /content/BoostYourOwnDepth/input/rgb ######################\n",
    "image_name = ''\n",
    "############################################################################\n",
    "\n",
    "image_path = os.path.join('/content/BoostYourOwnDepth/input/rgb',image_name)\n",
    "img = read_image(image_path)\n",
    "\n",
    "# Hyperparameters\n",
    "whole_size_threshold = 3000  # R_max from the paper\n",
    "r_threshold_value = 0.2 # Value x of R_x defined in the section 5 of the main paper.\n",
    "scale_threshold = 3  # Allows up-scaling with a scale up to 3\n",
    "\n",
    "##########################################################################\n",
    "##### Fill in the receptive field size of your network here ##############\n",
    "net_receptive_field_size = \n",
    "##########################################################################\n",
    "\n",
    "# Find the best input resolution R-x. The resolution search described in section 5-double estimation of the main paper and section B of the\n",
    "# supplementary material.\n",
    "whole_image_optimal_size, _ = calculateprocessingres(img, net_receptive_field_size,r_threshold_value, scale_threshold, whole_size_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3\n",
    "Put your low-res and high-res depth map in the corresponding folders in '/content/BoostYourOwnDepth/input/'.\n",
    "Then, you can run the following section to generate the merged estimation. The section executes a single pass through our merging network to create a double estimation from your inputs. **IMPORTANT**: Please make sure that high-res and low-res files have the same name to avoid reading errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code section 3\n",
    "!python3 /content/BoostYourOwnDepth/boost_depth.py --data_dir /content/BoostYourOwnDepth/input/ --output_dir /content/BoostYourOwnDepth/output --checkpoints_dir /content/BoostYourOwnDepth/pix2pix/checkpoints --colorize_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feel free to experiment!\n",
    "\n",
    "E.g. edit the input depth maps, mix'n'match different depth estimators or use your own model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
